{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Read CSV Write Parquet').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"input_files/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+---+-------+-------+---+------+\n",
      "|                 _c0|                 _c1|                 _c2|     _c3|_c4|    _c5|    _c6|_c7|   _c8|\n",
      "+--------------------+--------------------+--------------------+--------+---+-------+-------+---+------+\n",
      "|8e66dea62a914ba69...|2019-09-12T00:00:...|http://example.co...|user-028| ER| Chrome|    Mac|501|0.6975|\n",
      "|3897a5a7115d489e9...|2019-09-12T00:00:...|http://example.co...|user-032| SJ|Firefox|  Linux|307|0.3905|\n",
      "|baf865e583aa45f79...|2019-09-12T00:01:...|http://example.co...|user-031| MA|Firefox|  Linux|510|0.5118|\n",
      "|656bd941b94044a18...|2019-09-12T00:01:...|http://example.co...|user-057| GD|   Edge|  Linux|510|0.1552|\n",
      "|a4156ae1faf74acc9...|2019-09-12T00:02:...|http://example.co...|user-054| ZW|   Edge|Android|208|0.7311|\n",
      "+--------------------+--------------------+--------------------+--------+---+-------+-------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.parquet(\"hw5_problem2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"hw5_problem2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+---+-------+-------+---+------+\n",
      "|                 _c0|                 _c1|                 _c2|     _c3|_c4|    _c5|    _c6|_c7|   _c8|\n",
      "+--------------------+--------------------+--------------------+--------+---+-------+-------+---+------+\n",
      "|eefc2eb34a354ab7a...|2019-09-12T00:00:...|http://example.co...|user-041| CR|     IE|Android|201|0.2856|\n",
      "|9ba55c683ddd4a05a...|2019-09-12T00:00:...|http://example.co...|user-043| NE| Safari|windows|403|0.2843|\n",
      "|a45df30287454e598...|2019-09-12T00:01:...|http://example.co...|user-005| VI|   Edge|Android|444|0.6983|\n",
      "|941c2d880242431c9...|2019-09-12T00:01:...|http://example.co...|user-092| IS|Firefox|windows|226|0.2636|\n",
      "|97723d7e1669450eb...|2019-09-12T00:02:...|http://example.co...|user-024| GM|Firefox|windows|303|0.0459|\n",
      "+--------------------+--------------------+--------------------+--------+---+-------+-------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import date_trunc\n",
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = spark.read.load(\"hw5_problem2.parquet\")\n",
    "\n",
    "# reduce df by urls, country, hour\n",
    "dfReduced = df.select(from_unixtime(unix_timestamp('_c1', \"yyyy-MM-dd'T'HH:mm:ss\")).alias('datetime')\\\n",
    "                          ,col('_c4').alias('country')\\\n",
    "                          ,col('_c2').alias('url')\\\n",
    "                         )\n",
    "\n",
    "# specify datetime range\n",
    "t1 = '2019-09-13 17:00:00'\n",
    "t2 = '2019-09-14 09:00:00'\n",
    "\n",
    "# reduce by datetime range\n",
    "dfReduced = dfReduced.filter(dfReduced[\"datetime\"] >= lit(t1))\n",
    "dfReduced = dfReduced.filter(dfReduced[\"datetime\"] <= lit(t2))\n",
    "\n",
    "# add datehour\n",
    "#dfReduced = dfReduced.withColumn(  'datehour' , date_trunc(\"hour\", \"datetime\")  )\n",
    "dfReduced = dfReduced.withColumn(  'datehour' , date_format(col(\"datetime\"), \"yyyy-MM-dd:HH\") )\n",
    "\n",
    "dfQueryFour = dfReduced.select('datehour','country','url').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfQueryFour.orderBy('datehour', ascending=True).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------+\n",
      "|     datehour|country|count(1)|\n",
      "+-------------+-------+--------+\n",
      "|2019-09-13:17|     AD|       2|\n",
      "|2019-09-13:17|     AE|       6|\n",
      "|2019-09-13:17|     AF|       1|\n",
      "|2019-09-13:17|     AI|       2|\n",
      "|2019-09-13:17|     AL|       1|\n",
      "+-------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQueryFour.groupBy(\"datehour\", \"country\").agg(count(\"*\")).orderBy('datehour','country', ascending=True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "df.withColumn(\"dateColumn\",  date_format(col(\"vacationdate\"), \"yyyy-MM-dd\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
